{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of job_word_compiler.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2SKmgz7rp_"
      },
      "source": [
        " # import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmmF_gJ4k6hv"
      },
      "source": [
        "# requests lib to handle the GET requests\n",
        "import requests\n",
        "# beautiful soup to parse the html from the GET response\n",
        "from bs4 import BeautifulSoup as bsoup\n",
        "# regex to format and clean text strings\n",
        "import re\n",
        "# time module used to mark the processing start time\n",
        "from time import time\n",
        "# sleep function used to limit the request cycle time to avoid overloading the server\n",
        "from time import sleep\n",
        "\n",
        "# used to monitor the processing stats during runtime\n",
        "from IPython.core.display  import clear_output\n",
        "\n",
        "# authentification for access and writing to google drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# used to manipulate date into easy to convert forms for storage\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4SoY9R73Du"
      },
      "source": [
        "# set initial variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k6vbFAiv3Wv"
      },
      "source": [
        "# setting the url query variables to be used in get request\n",
        "query = 'data+scientist'\n",
        "location = 'United+States'\n",
        "job_type = 'fulltime'\n",
        "level = 'entry_level'\n",
        "start = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu3ygH75k_vV"
      },
      "source": [
        "# set the base url for the request\n",
        "url = 'https://www.indeed.com/jobs'\n",
        "payload = {\n",
        "    'q' : query,\n",
        "    'l' : location,\n",
        "    'jt' : job_type,\n",
        "    'explvl' : level,\n",
        "    'start' : str(start)\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2Hs_uM078iZ"
      },
      "source": [
        "# data retrieval function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHATTVHwl3IE",
        "outputId": "b4092321-18a0-4eae-ed9d-8f375a43e868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "response = requests.get(url, params=payload)\n",
        "# creates a new list for the split text fragments\n",
        "\n",
        "new_list = []\n",
        "start_time = time()\n",
        "pages = 0\n",
        "limit = 5\n",
        "\n",
        "while pages < limit:\n",
        "  clear_output(wait=True)\n",
        "\n",
        "  if (response.ok):\n",
        "    data = response.text\n",
        "    soup = bsoup(data, 'html.parser')\n",
        "\n",
        "  # code for extracting the number of pages in the search and setting it equal to the max limit\n",
        "  # find the div containing the page number our of the total number of pages\n",
        "  page_count_raw = soup.find('div', id = 'searchCountPages')\n",
        "\n",
        "  # convert the div contents to a string and separate by spaces\n",
        "  page_count_split = re.split(' ', str(page_count_raw))\n",
        "\n",
        "  # set page count equal to the second to last item in the split list which is a string representing the total number of pages\n",
        "  page_count = page_count_split[-2]\n",
        "\n",
        "  # remove the comma from the string so that it can be converted to an int\n",
        "  page_count = int(re.sub(',', '', page_count))\n",
        "  max_limit = page_count\n",
        "  limit = max_limit\n",
        "\n",
        "  # if the page limit used is greater than the total number of pages available then the limit is set to the max number\n",
        "  if limit > max_limit:\n",
        "    limit = max_limit\n",
        "\n",
        "  # separate the html data into a list of items with the div tag and 'summary' class\n",
        "  summary_div = soup.find_all('div', class_ = 'summary')\n",
        "  #print(len(summary_div))\n",
        "\n",
        "\n",
        "  # create new list to add new list elements from the div list to\n",
        "  summary_list = []\n",
        "  word_list = []\n",
        "  # iterates though each div containter and finds the <li> elements and adds them to the new list\n",
        "  for summary in summary_div:\n",
        "    summary_list.append(summary.find_all('li'))\n",
        "\n",
        "  # iterates though the summary_list and splits them at the '>' tag character to remove tag characters from the front of the text that we want\n",
        "  for item in summary_list:\n",
        "    for re_item in item:\n",
        "      re_item = str(re_item)\n",
        "      sub_one = re.sub('<b>', '', re_item)\n",
        "      #print(sub_one)\n",
        "      sub_two = re.sub('</b>', '', sub_one)\n",
        "      #print(sub_two)\n",
        "      sub_three = re.sub('<li style=\"margin-bottom:0px;\">', '', sub_two)\n",
        "      #print(sub_three)\n",
        "      sub_four = re.sub('</li>', '', sub_three)\n",
        "      #print(sub_four)\n",
        "      sub_five = re.sub('<li>', '', sub_four)\n",
        "      #print(sub_five)\n",
        "      new_list.append(sub_five[0:-1])\n",
        "      final_split = re.split(' ', sub_five[0:-1])\n",
        "      for word in final_split:\n",
        "        word_list.append(word)\n",
        "  \n",
        "  sleep(0.5)\n",
        "  \n",
        "  # output some logs for monitoring\n",
        "  elapsed_time = time() - start_time\n",
        "  pages += 1\n",
        "  print('Requests: {}, Frequency: {} requests/s, {} strings processed.'.format(pages, pages/elapsed_time, len(new_list)))\n",
        "  start += 10\n",
        "  \n",
        "  print(str(pages)+' pages processed...')\n",
        "\n",
        "print('Processing complete!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requests: 6689, Frequency: 1.5746016815345876 requests/s, 207359 strings processed.\n",
            "6689 pages processed...\n",
            "Processing complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOeK3CGG8OEr"
      },
      "source": [
        "# data writing and storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8SMYPy-R_i-"
      },
      "source": [
        "# install pydrive for writing data to csv\n",
        "\n",
        "!pip install -U -q PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLibet4qSO3s"
      },
      "source": [
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djEmDwFZrT39"
      },
      "source": [
        "# turns the new_list list variable into a pandas series\n",
        "new_list_array = pd.Series(new_list)\n",
        "\n",
        "# the newly created 1 dimensional array is converted to a csv file with the pandas .to_csv() method function\n",
        "new_list_array.to_csv('pd_job_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xX6qIr9urgX"
      },
      "source": [
        "# new list variable created so that when the individual list strings in the new_list variable are split into a list of word strings, they can be added to this new list of lists\n",
        "new_list_split = []\n",
        "\n",
        "# the individual string lists are then formatted a little\n",
        "for line in new_list:\n",
        "  # all commas in the strings are removed\n",
        "  new_line = re.sub(',', '', line)\n",
        "  # each sentence string is then split into a list of the individual words\n",
        "  split_line = re.split(' ', new_line)\n",
        "  # each list of words is then added to our new_list_split list \n",
        "  new_list_split.append(split_line)\n",
        "\n",
        "# the list of word lists are then made into a data frame variable in pandas\n",
        "pd_word_list = pd.DataFrame(new_list_split)\n",
        "\n",
        "# the dataframe variable is then saved to a csv file with each sentence on a separate row and a separate column cell for each word in that sentence\n",
        "pd_word_list.to_csv('pd_job_data_words.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}